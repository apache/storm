########### These MUST be filled in for a storm configuration
 storm.zookeeper.servers:
     - "localhost"

 storm.zookeeper.root: "/jstorm"
 
 #nimbus.host is being used by $JSTORM_HOME/bin/start.sh
 #it only support IP, please don't set hostname
 # For example 
 # nimbus.host: "10.132.168.10, 10.132.168.45"
 #nimbus.host: "localhost"
 
# %JSTORM_HOME% is the jstorm home directory
 storm.local.dir: "%JSTORM_HOME%/data"
 
 java.library.path: "/usr/local/lib:/opt/local/lib:/usr/lib"



# if supervisor.slots.ports is null, 
# the port list will be generated by cpu cores and system memory size 
# for example, if there are 24 cpu cores and supervisor.slots.port.cpu.weight is 1.2
# then there are 24/1.2 ports for cpu, 
# there are system_physical_memory_size/worker.memory.size ports for memory 
# The final port number is min(cpu_ports, memory_port)
 supervisor.slots.ports.base: 6800
 supervisor.slots.port.cpu.weight: 1
 supervisor.slots.ports: null
#supervisor.slots.ports:
#    - 6800
#    - 6801
#    - 6802
#    - 6803

# Default disable user-define classloader
# If there are jar conflict between jstorm and application, 
# please enable it 
 topology.enable.classloader: false

# enable supervisor use cgroup to make resource isolation
# Before enable it, you should make sure:
# 	1. Linux version (>= 2.6.18)
# 	2. Have installed cgroup (check the file's existence:/proc/cgroups)
#	3. You should start your supervisor on root
# You can get more about cgroup:
#   http://t.cn/8s7nexU
 supervisor.enable.cgroup: false


### Netty will send multiple messages in one batch  
### Setting true will improve throughput, but more latency
 storm.messaging.netty.transfer.async.batch: true

### if this setting  is true, it will use disruptor as internal queue, which size is limited
### otherwise, it will use LinkedBlockingDeque as internal queue , which size is unlimited
### generally when this setting is true, the topology will be more stable,
### but when there is a data loop flow, for example A -> B -> C -> A
### and the data flow occur blocking, please set this as false
 topology.buffer.size.limited: true
 
### default worker memory size, unit is byte
 worker.memory.size: 2147483648

# Metrics Monitor
# topology.performance.metrics: it is the switch flag for performance 
# purpose. When it is disabled, the data of timer and histogram metrics 
# will not be collected.
# topology.alimonitor.metrics.post: If it is disable, metrics data
# will only be printed to log. If it is enabled, the metrics data will be
# posted to alimonitor besides printing to log.
 topology.performance.metrics: true
 topology.alimonitor.metrics.post: false

# UI MultiCluster
# Following is an example of multicluster UI configuration
# ui.clusters:
#     - {
#         name: "jstorm",
#         zkRoot: "/jstorm",
#         zkServers:
#             [ "localhost"],
#         zkPort: 2181,
#       }
